<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ajcobraa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ajcobraa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-25T06:08:49+00:00</updated><id>https://ajcobraa.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">From Science Fiction to Reality The AI Revolution from Neural Networks to GPT</title><link href="https://ajcobraa.github.io/blog/2025/aievolution/" rel="alternate" type="text/html" title="From Science Fiction to Reality The AI Revolution from Neural Networks to GPT"/><published>2025-03-24T17:39:00+00:00</published><updated>2025-03-24T17:39:00+00:00</updated><id>https://ajcobraa.github.io/blog/2025/aievolution</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2025/aievolution/"><![CDATA[ <h2 id="early-neural-networks-and-backpropagation-1980s">Early Neural Networks and Backpropagation (1980s):</h2> <p>The concept of artificial neural networks (ANNs) dates back to the mid-20th century. Early models like Frank Rosenblatt’s perceptron (1958) could learn simple patterns, but they were limited in scope. In 1969, Marvin Minsky and Seymour Papert published Perceptrons, highlighting that single-layer networks can only learn linearly separable functions, which caused a decline in neural network research in the 1970s​ <a href="https://www.techtarget.com/whatis/feature/History-and-evolution-of-machine-learning-A-timeline#:~:text=for%20deep%20learning"><code class="language-plaintext highlighter-rouge">techttarget</code></a> . The field rebounded in the 1980s with the introduction of the backpropagation algorithm (pioneered by Rumelhart, Hinton, Williams, and others). Backpropagation enabled efficient training of multi-layer networks (i.e. networks with “hidden” layers), an advancement over the single-layer perceptron and a foundational breakthrough for deep learning​ . This meant ANNs could now learn much more complex mappings from inputs to outputs. However, training these early networks was computationally expensive and often limited by the hardware and data available at the time, so practical impact remained modest until later.</p> <pre><code class="language-mermaid">flowchart TD
    subgraph Input_Layer
        I1[Input 1]
        I2[Input 2]
        I3[Input 3]
    end

    subgraph Hidden_Layer
        H1[Hidden 1]
        H2[Hidden 2]
        H3[Hidden 3]
    end

    subgraph Output_Layer
        O1[Output 1]
        O2[Output 2]
    end

    I1 --&gt; H1
    I1 --&gt; H2
    I1 --&gt; H3
    I2 --&gt; H1
    I2 --&gt; H2
    I2 --&gt; H3
    I3 --&gt; H1
    I3 --&gt; H2
    I3 --&gt; H3

    H1 --&gt; O1
    H1 --&gt; O2
    H2 --&gt; O1
    H2 --&gt; O2
    H3 --&gt; O1
    H3 --&gt; O2
</code></pre> <hr/> <h2 id="recurrent-neural-networks-and-long-term-dependencies-1990s">Recurrent Neural Networks and Long-Term Dependencies (1990s)</h2> <p>As researchers explored new architectures, they developed Recurrent Neural Networks (<a href="https://arxiv.org/abs/1912.05911"><code class="language-plaintext highlighter-rouge">RNNs</code></a>) to handle sequential data (like time series and language). RNNs introduce feedback loops so that past outputs can influence future inputs, giving the network a form of memory. In theory this allowed modeling long-term dependencies, but in practice RNNs were very hard to train on long sequences due to the vanishing gradient problem: as errors were back-propagated through many time steps, the gradient values would shrink exponentially, preventing the network from learning long-range correlations​. In the mid-1990s, this limitation was addressed by the invention of the Long Short-Term Memory (LSTM) network (Hochreiter &amp; Schmidhuber, 1997). LSTM introduced gating mechanisms that let the network decide what to keep or forget, enabling it to preserve information over hundreds or thousands of time steps​. This was a major breakthrough for sequence learning – LSTMs overcame vanishing gradients and soon “set accuracy records in multiple application domains”, becoming the default RNN architecture for tasks like speech recognition​ <a href="https://en.wikipedia.org/wiki/History_of_artificial_neural_networks#Recurrent_network_architectures#:~:text=Long%20short,default%20choice%20for%20RNN%20architecture"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a> . Thanks to LSTM (and later, the simplified GRU variant), RNNs in the 2000s could learn long-term patterns in text, audio, and other sequence data that vanilla RNNs could not.</p> <hr/> <h2 id="convolutional-neural-networks-and-the-deep-learning-boom-2010s">Convolutional Neural Networks and the Deep Learning Boom (2010s)</h2> <p>Early vs. modern convolutional networks: The diagram compares the 1990s LeNet-5 CNN (left) to the deeper 2012 AlexNet CNN (right), illustrating how neural networks grew in depth and complexity as more data and compute became available​ <a href="https://en.wikipedia.org/wiki/AlexNet#:~:text=While%20AlexNet%20and%20LeNet%20share,18"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a></p> <p>![Comparison of LeNet-5 and AlexNet architectures]<a href="https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg.jpg" data-lightbox="roadtrip"><img src="https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg"/></a> “Diagram comparing the architectures of LeNet-5 (1998) and AlexNet (2012), showing the increase in depth and complexity.”</p> <p><em>Caption: A comparison of LeNet-5 (left) and AlexNet (right), highlighting the evolution of CNN architectures with increased depth and complexity in the 2010s.</em> . Convolutional Neural Networks (<a href="https://arxiv.org/abs/1511.08458"><code class="language-plaintext highlighter-rouge">CNNs</code></a>) were designed to process grid-like data such as images. Inspired by the human visual cortex, CNNs use layers of convolution filters to automatically learn spatial hierarchies of features (edges, shapes, objects) from images. Early CNN work by Yann LeCun in the late 1980s demonstrated that neural nets could recognize handwritten characters (e.g. zip code digits) with high accuracy​. LeCun’s LeNet-5 (1998) is a landmark example, successfully reading handwritten digits. Despite these successes, throughout the 1990s and early 2000s neural networks often failed to outperform more hand-crafted approaches (like SVMs or decision trees) on complex tasks, and many in the research community were skeptical​. Two factors changed this: data and hardware. By the 2010s, much larger labeled datasets became available (e.g. the ImageNet database of millions of images), and graphics processing units (GPUs) allowed the training of bigger, deeper networks. In 2012, a deep CNN known as AlexNet (developed by Hinton, Krizhevsky, and Sutskever) won the ImageNet competition by a huge margin, beating the previous state-of-the-art in image recognition by a significant leap​. This triumph vividly showed the power of deep learning and “triggered an explosion of deep learning research and implementation” across the industry​ <a href="https://www.techtarget.com/whatis/feature/History-and-evolution-of-machine-learning-A-timeline#:~:text=2012"><code class="language-plaintext highlighter-rouge">techttarget</code></a> . CNNs rapidly became the dominant approach for computer vision tasks – from object detection to facial recognition – and were soon achieving superhuman performance in some domains (e.g. recognizing traffic signs or diagnosing medical images). The success of AlexNet and subsequent CNN models (VGG, GoogLeNet, ResNet, etc.) marks the beginning of the modern deep learning era in AI.</p> <hr/> <h2 id="generative-adversarial-networks-2014--generative-ai-breakthrough">Generative Adversarial Networks (2014) – Generative AI Breakthrough</h2> <p>As neural networks grew more capable, researchers began exploring models that generate new data rather than just recognize it. A milestone in this area was the introduction of Generative Adversarial Networks (GANs) in 2014 by Ian Goodfellow and colleagues​ <a href="https://arxiv.org/abs/1406.2661"><code class="language-plaintext highlighter-rouge">GAN</code></a> . A GAN consists of two competing neural networks: a generator that tries to create realistic fake data (e.g. images that look like real photographs), and a discriminator that tries to tell apart the generator’s fakes from real data. The two networks are trained in tandem in a “adversarial” game: the generator improves at fooling the discriminator, and the discriminator gets better at spotting fakes. This clever setup enabled GANs to produce incredibly realistic images, noises, and other data. Over the 2014–2018 period, GANs became state-of-the-art in generative modeling​ <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a> – for example, NVIDIA’s StyleGAN (2018) could generate photorealistic human faces that don’t belong to any real person​ <a href="https://research.nvidia.com/publication/2022-05_stylegan-nada-clip-guided-domain-adaptation-image-generators"><code class="language-plaintext highlighter-rouge">research.nvidia</code></a> . Researchers also built variants for tasks like image-to-image translation (e.g. turning sketches into color images) and video generation. The advent of GANs was significant because it showed neural networks could create – not just classify – complex data, opening the door to today’s generative AI applications (art creation, deepfakes, data augmentation, etc.). GANs did have their challenges (such as training instabilities like mode collapse), but they undeniably pushed the boundaries of what AI could do, and spurred a lot of excitement about generative AI.</p> <pre><code class="language-mermaid">stateDiagram-v2
    [*] --&gt; Noise
    Noise --&gt; Generator : Random Vector z
    Generator --&gt; Fake_Data : Generates Fake Samples
    Real_Data --&gt; Discriminator
    Fake_Data --&gt; Discriminator
    Discriminator --&gt; Decision : Real / Fake?
    Decision --&gt; Generator : Feedback (Loss)
    Decision --&gt; Discriminator : Feedback (Loss)
    Decision --&gt; [*]

    state Real_Data {
        direction LR
        Dataset --&gt; Sample
    }
</code></pre> <p>Explanation:</p> <p>Noise: Random input vector (z) fed to the Generator.</p> <p>Generator: Creates fake data samples from noise.</p> <p>Real_Data: Authentic samples from the real dataset.</p> <p>Discriminator: Classifies input samples as real or fake.</p> <p>Decision: Discriminator’s classification result.</p> <p>Feedback (Loss): Used to improve both Generator and Discriminator.</p> <hr/> <p>Transformers – “Attention Is All You Need” (2017)</p> <p>In 2017, a single paper fundamentally altered the course of AI research. Google researchers Ashish Vaswani et al. published “Attention Is All You Need,” which introduced the Transformer architecture​ <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need#:~:text=,was%20on%20improving%20%2060"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a> . The Transformer was a radical departure from previous sequence models (like RNNs) because it did not use any recurrent loops or convolutions for processing sequences. Instead, it relied entirely on a mechanism called self-attention that allowed the model to weigh the influence of different input elements on each other. In essence, a Transformer can look at an entire sequence (say, a sentence) and learn which words or parts of the sequence are relevant to each other, and it can do this for all parts of the sequence in parallel​ <a href="https://arxiv.org/abs/1706.03762"><code class="language-plaintext highlighter-rouge">Transformers</code></a> . By removing the sequential nature of RNNs, Transformers achieved two huge benefits: they preserved long-range context much more effectively (since even distant elements can directly attend to each other), and they could be parallelized during training, making it feasible to train very large models on GPUs or TPUs​ <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need#:~:text=The%20paper%20is%20most%20well,bigger%20sizes%20to%20be%20trained"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a> . The 2017 paper focused on machine translation, showing that Transformers outperformed RNN-based translation models and were faster to train​. But the authors correctly anticipated that the approach could be applied much more broadly​. Indeed, the Transformer architecture quickly became the new standard for natural language processing. It was modular (built from repeated self-attention and feed-forward layers) and scaled extremely well with data and compute. Almost overnight, researchers began replacing recurrent networks with Transformers in language tasks. The Transformer is now regarded as a “foundational” architecture in modern AI, forming the backbone of most large-scale models​. With Transformers, the stage was set for an era of unprecedented progress, as they enabled the training of models on unthinkably large datasets and for new capabilities.</p> <hr/> <p>Large Language Models and Modern AI Explosion (2018–Present)</p> <p>The introduction of Transformers in 2017 ushered in a period of accelerating progress in AI. One of the first major fruits of this innovation was the rise of large language models (LLMs) built on the Transformer architecture. In 2018, OpenAI released the first Generative Pre-Trained Transformer model, known as GPT-1, demonstrating that a Transformer trained on massive amounts of text could generate fluent passages of natural language​ <a href="https://www.techtarget.com/whatis/feature/History-and-evolution-of-machine-learning-A-timeline#:~:text=2018"><code class="language-plaintext highlighter-rouge">techttarget</code></a> . OpenAI’s approach was to pre-train the model on a diverse corpus (so it learned general language patterns) and then fine-tune it on specific tasks – a recipe that proved extremely powerful. The following year, they unveiled GPT-2 (2019), a significantly larger model with about 1.5 billion parameters​ <a href="https://en.wikipedia.org/wiki/GPT-3#:~:text=of%20manually,9"><code class="language-plaintext highlighter-rouge">Wikipedia</code></a> . GPT-2 showed dramatic improvements in generating coherent and contextually relevant text, to the point of writing entire news articles or stories that were often hard to distinguish from human-written content. In 2020 came GPT-3, which was a quantum leap in scale: 175 billion parameters​ trained on nearly all of the Internet’s text. GPT-3 astonished the world with its ability to perform tasks it was never explicitly trained for (from writing code to answering complex questions) simply by being prompted with a few examples – so-called “few-shot” learning​ <a href="https://www.ibm.com/think/topics/few-shot-learning#:~:text=Few%2Dshot%20learning%20is%20a,suitable%20training%20data%20is%20scarce."><code class="language-plaintext highlighter-rouge">few-shot</code></a> . This demonstrated a surprising emergent capability: with enough data and parameters, a Transformer-based model could learn to do a wide range of tasks without specialized training for each one. Other organizations and researchers also quickly adopted Transformers to build their own large models. In late 2018 Google introduced BERT (Bidirectional Encoder Representations from Transformers), which used a Transformer encoder to achieve state-of-the-art results in language understanding tasks. Subsequent models like MegatronLM, T5, XLNet, and PALM pushed the envelope further in various ways (larger sizes, different training objectives, etc.). These models collectively are referred to as large language models, and they began to revolutionize language-based AI across the board – powering translation services, search engines, personal assistants, and more. A key turning point in public awareness was the release of ChatGPT by OpenAI in late 2022. ChatGPT took the GPT-3.5 model and fine-tuned it for conversational interaction, packaging it in an accessible chat interface. Within months, it reached over 100 million users, exposing millions of people to AI-generated text as a useful (and sometimes uncanny) tool in daily life​. Shortly after, in 2023, OpenAI announced GPT-4, a still larger and more advanced model that is multimodal (able to accept text and images as input) and exhibits even more advanced reasoning and understanding capabilities​. The pace at which these Transformer-based systems have improved is often described as exponential. In fact, the period from 2017 to 2025 has seen more rapid AI development than perhaps the entire 30 years prior – a testament to how impactful the Transformer innovation has been. Improved algorithms (the attention-based architectures), orders-of-magnitude more computing power, and unprecedented quantities of training data have together “fueled a revolution” in machine learning, resulting in rapid improvements on many tasks​. Today’s AI models can converse, answer questions, generate images, compose music, and more – achievements that would have been science fiction a decade ago. And this progress shows no sign of slowing down, all tracing back to those key milestones: from the early neural nets and CNNs, to GANs, and especially the game-changing introduction of Transformers in 2017 that truly transformed the evolution of AI.</p> ]]></content><author><name>Anupam Jose</name></author><category term="AI-Evolution"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Photo Gallery</title><link href="https://ajcobraa.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="Photo Gallery"/><published>2024-12-04T00:00:00+00:00</published><updated>2024-12-04T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/photo-gallery/"><![CDATA[ <p> &lt;!– —</p> <h2 id="spotlight-js-"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a> –&gt;</h2> ]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– The images in this post are all zoomable, arranged into different mini-galleries using different libraries.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://ajcobraa.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Tabs</title><link href="https://ajcobraa.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="Tabs"/><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/tabs</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/tabs/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with tabs date: 2024-05-01 00:32:13 description: this is what included tabs in a post could look like tags: formatting code categories: sample-posts tabs: true —]]></summary></entry><entry><title type="html">Typograms</title><link href="https://ajcobraa.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="Typograms"/><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/typograms</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/typograms/"><![CDATA[ <p>My first diagram! +—-+</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>. –&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with typograms date: 2024-04-29 23:36:10 description: this is what included typograms code could look like tags: formatting diagrams categories: sample-posts typograms: true —]]></summary></entry><entry><title type="html">Post Citation</title><link href="https://ajcobraa.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="Post Citation"/><published>2024-04-28T00:00:00+00:00</published><updated>2024-04-28T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/post-citation/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post that can be cited date: 2024-04-28 15:06:00 description: this is what a post that can be cited looks like tags: formatting citation categories: sample-posts citation: true —]]></summary></entry><entry><title type="html">Pseudocode</title><link href="https://ajcobraa.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="Pseudocode"/><published>2024-04-15T00:00:00+00:00</published><updated>2024-04-15T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/pseudocode/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with pseudo code date: 2024-04-15 00:01:00 description: this is what included pseudo code could look like tags: formatting code categories: sample-posts pseudocode: true —]]></summary></entry><entry><title type="html">Advanced Images</title><link href="https://ajcobraa.github.io/blog/2024/advanced-images/" rel="alternate" type="text/html" title="Advanced Images"/><published>2024-01-27T00:00:00+00:00</published><updated>2024-01-27T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/advanced-images</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/advanced-images/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

&lt;img
  src="/assets/img/9.jpg"
  
    class="img-fluid rounded z-depth-1"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
  
  
    loading="eager"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt; &lt;/swiper-slide&gt;</p> <swiper-slide> <figure> <picture> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <p>&lt;/swiper-container&gt;</p> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <p>–&gt;</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with advanced image components date: 2024-01-27 11:46:00 description: this is what advanced image components could look like tags: formatting images categories: sample-posts thumbnail: assets/img/9.jpg images: compare: true slider: true]]></summary></entry><entry><title type="html">Code Diff</title><link href="https://ajcobraa.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="Code Diff"/><published>2024-01-27T00:00:00+00:00</published><updated>2024-01-27T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/code-diff/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with code diff date: 2024-01-27 19:22:00 description: this is how you can display code diffs tags: formatting code categories: sample-posts code_diff: true]]></summary></entry><entry><title type="html">Vega Lite</title><link href="https://ajcobraa.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="Vega Lite"/><published>2024-01-27T00:00:00+00:00</published><updated>2024-01-27T00:00:00+00:00</updated><id>https://ajcobraa.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://ajcobraa.github.io/blog/2024/vega-lite/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– — layout: post title: a post with vega lite date: 2024-01-27 00:20:00 last_updated: 2024-04-14 04:30:00 description: this is what included vega lite code could look like tags: formatting charts categories: sample-posts chart: vega_lite: true]]></summary></entry></feed>